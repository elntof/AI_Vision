### 1. 기본 설정 & 유틸
import os, time, math, random
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torchvision import transforms, datasets

from tqdm.auto import tqdm
import matplotlib.pyplot as plt

def seed_everything(seed: int = 42):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

seed_everything(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[INFO] device: {device} | CUDA: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"[INFO] GPU: {torch.cuda.get_device_name(0)}")

### 2. 경로 & 하이퍼파라미터
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image
import torch

# ------- ROI 지정 -------
ROI_RECT   = (57, 0, 175, 555)   # (x, y, w, h) 사용자 지정
ROI_COLOR  = "lime"
ROI_EDGE_LW= 2.0

DATA_ROOT = r"D:\Python\3. Deep Learning\1. Icing Classification_ViT\rawdata"

def show_roi_overlay(img_path: Path, roi_xywh, color=ROI_COLOR, edge_lw=ROI_EDGE_LW):
    """ROI 프리뷰"""
    if not img_path.exists():
        print(f"[WARN] ROI demo image not found: {img_path}")
        return
    img = Image.open(img_path).convert("RGB")
    W, H = img.size
    x, y, w, h = map(int, roi_xywh)
    x = max(0, min(x, W - 1)); y = max(0, min(y, H - 1))
    w = max(1, min(w, W - x)); h = max(1, min(h, H - y))

    fig, ax = plt.subplots(figsize=(6, 6))
    ax.imshow(img)
    rect = patches.Rectangle((x, y), w, h, linewidth=edge_lw,
                             edgecolor=color, facecolor='none')
    ax.add_patch(rect)
    ax.set_title(f"ROI [x={x}, y={y}, w={w}, h={h}]")
    ax.set_axis_off(); ax.set_aspect('equal', adjustable='box')
    plt.show()

# 예시 프리뷰 (ROI 오버레이 확인을 원하면 주석 해제)
# demo_path = Path(DATA_ROOT) / "Icing" / "X7_WX7VP_REMELT_1_06m30s_005850.jpg"
# show_roi_overlay(demo_path, ROI_RECT)

# ------- 하이퍼파라미터 -------
IMG_SIZE         = 224
EPOCHS           = 20
WARMUP_EPOCHS    = 2
BATCH_SIZE       = 32
BASE_LR          = 5e-5
WEIGHT_DECAY     = 0.05
NUM_WORKERS      = 0
PIN_MEMORY       = torch.cuda.is_available()

# ------- 모델 -------
MODEL_NAME       = "vit_small_patch16_224"
USE_PRETRAINED   = True
DROP_PATH_RATE   = 0.1

# 체크포인트
SAVE_BEST_PATH   = "vit_icing_melted_standard_best.pth"

# 배치 제한(디버그용)
LIMIT_TRAIN_BATCHES = None
LIMIT_VAL_BATCHES   = None

assert Path(DATA_ROOT).exists(), f"DATA_ROOT not found: {DATA_ROOT}"

### 2-1. 임베딩 기반 중복 이미지 제거 (클래스별)
import os, re, math, gc, random, numpy as np, torch, logging, warnings
from pathlib import Path
from PIL import Image, ImageOps, ImageDraw
from IPython.display import display
from torchvision import transforms
import timm

#  timm 경고/로그 억제
warnings.filterwarnings("ignore", message=".*Unexpected keys.*")
logging.getLogger("timm").setLevel(logging.ERROR)
logging.getLogger("timm.models").setLevel(logging.ERROR)

# 최소 설정
DATA_ROOT   = Path(globals().get("DATA_ROOT", "."))
ROI_RECT    = tuple(globals().get("ROI_RECT", (0,0,224,224)))
IMG_SIZE    = int(globals().get("IMG_SIZE", 224))

# 클래스 이름 안전하게 확보
if "dataset_full" in globals() and hasattr(globals()["dataset_full"], "classes"):
    CLASSES = list(globals()["dataset_full"].classes)
elif "idx_to_class" in globals():
    CLASSES = sorted(set(globals()["idx_to_class"].values()))
else:
    CLASSES = [d.name for d in DATA_ROOT.iterdir() if d.is_dir()]
CLASSES = [c for c in CLASSES if (DATA_ROOT / str(c)).exists()]
CLASSES.sort()

# ------- 임베딩/클러스터링 파라미터 -------
SIM_THR_DEFAULT = 0.992   # 높을수록 더 엄격 (0.985~0.995 권장)
NN_K_DEFAULT    = 10      # 최근접 이웃 수 (연결 후보 수)
K_PER_CLUSTER   = 1       # 클러스터당 남길 'keep' 수 (보통 1)

# 기본값 유지 + 외부 오버라이드 수용
PER_CLASS_SIM_THR = globals().get("PER_CLASS_SIM_THR", {"Icing": 0.975, "Melted": 0.985})
PER_CLASS_NN_K    = globals().get("PER_CLASS_NN_K",    {"Icing": 10,    "Melted": 10})

# ------- 프리뷰 레이아웃 -------
COLS       = 20                 # 20열
TILE_W     = 120
TILE_H     = 180
BG_COLOR   = (0,0,0)
SHEET_MAX_ROWS = 40             # 페이지당 최대 행

# 변환
imagenet_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
imagenet_std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)

class CropROI:
    def __init__(self, xywh): self.x, self.y, self.w, self.h = map(int, xywh)
    def __call__(self, img: Image.Image):
        img = ImageOps.exif_transpose(img)
        W, H = img.size
        x = max(0, min(self.x, W-1)); y = max(0, min(self.y, H-1))
        w = max(1, min(self.w, W-x));  h = max(1, min(self.h, H-y))
        return img.crop((x, y, x+w, y+h))

tfm_embed = transforms.Compose([
    CropROI(ROI_RECT),
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.ToTensor(),
    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),
])

def _natural_key(s: str):
    base = os.path.basename(str(s))
    return [int(t) if t.isdigit() else t.lower() for t in re.split(r"(\d+)", base)]

# 파일 수집
EXTS = {".jpg",".jpeg",".png",".bmp",".tif",".tiff"}
by_class = {}
for cls in CLASSES:
    paths = []
    for p in (DATA_ROOT/str(cls)).rglob("*"):
        if p.is_file() and p.suffix.lower() in EXTS:
            paths.append(str(p.resolve()))
    paths.sort(key=_natural_key)
    by_class[cls] = paths

# 클래스별 파일 수 출력
print("[INFO] files per class:")
for cls in CLASSES:
    print(f"  - {cls}: {len(by_class.get(cls, []))}")

# 임베딩 모델
DEVICE    = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_EMB = 64 if torch.cuda.is_available() else 16
feat_model = timm.create_model('vit_small_patch16_224', pretrained=True,
                               num_classes=0, global_pool='avg').to(DEVICE).eval()

@torch.no_grad()
def embed_paths(path_list):
    if len(path_list) == 0:
        return np.empty((0, 384), dtype=np.float32)  # vit_small 기준
    feats = []
    for s in range(0, len(path_list), BATCH_EMB):
        chunk = path_list[s:s+BATCH_EMB]
        xs = []
        for p in chunk:
            with Image.open(p) as img:
                xs.append(tfm_embed(img))
        x = torch.stack(xs, 0).to(DEVICE)
        f = feat_model(x).float().cpu()
        feats.append(f)
        del x, f, xs
    feats = torch.cat(feats, 0).numpy()
    feats = feats / (np.linalg.norm(feats, axis=1, keepdims=True) + 1e-12)
    return feats.astype(np.float32)

def union_find_clusters(feats, sim_thr=SIM_THR_DEFAULT, nn_k=NN_K_DEFAULT):
    N = feats.shape[0]
    parent = list(range(N))
    def find(a):
        while parent[a] != a:
            parent[a] = parent[parent[a]]
            a = parent[a]
        return a
    def union(a,b):
        ra, rb = find(a), find(b)
        if ra != rb: parent[rb] = ra
    if N == 0: return parent, {}
    try:
        from sklearn.neighbors import NearestNeighbors
        nbrs = NearestNeighbors(n_neighbors=min(nn_k,N), metric='cosine').fit(feats)
        dist, idxs = nbrs.kneighbors(feats, return_distance=True)
        sims = 1.0 - dist
        for i in range(N):
            for j, sim in zip(idxs[i], sims[i]):
                if i != j and sim >= sim_thr:
                    union(i, j)
    except Exception:
        blk = 2048
        for s in range(0, N, blk):
            e = min(N, s+blk)
            sim_blk = feats[s:e] @ feats.T
            for ii in range(s, e):
                row = sim_blk[ii-s]
                js = np.where(row >= sim_thr)[0]
                for j in js:
                    if j != ii:
                        union(ii, j)
        del sim_blk; gc.collect()
    clusters = {}
    for i in range(N):
        r = find(i)
        clusters.setdefault(r, []).append(i)
    return parent, clusters

def pick_keep_indices(paths, clusters, k=K_PER_CLUSTER):
    keep = []
    for _, members in clusters.items():
        ms = sorted(members, key=lambda m: _natural_key(paths[m]))
        keep.extend(ms[:k])
    return sorted(set(keep))

def parse_time_seconds_from_name(path: str):
    """시간 파싱 (이른 샷 먼저 정렬)"""
    base = os.path.basename(path)
    m = re.search(r'(\d{1,2})m(\d{1,2})s', base, flags=re.IGNORECASE)
    if m:
        return int(m.group(1)) * 60 + int(m.group(2))
    m2 = re.search(r'(\d{5,})\D*$', base)
    if m2:
        try:
            return float(m2.group(1))
        except:
            pass
    # 폴백
    try:
        return Path(path).stat().st_mtime
    except Exception:
        return float('inf')

def make_tile(path, label_text=None, tw=TILE_W, th=TILE_H):
    """텍스트 생성 (Ref 이미지만 / 상단 중앙 배치)"""
    with Image.open(path) as im:
        im = ImageOps.exif_transpose(im)
        x,y,w,h = ROI_RECT
        W,H = im.size
        x = max(0, min(x, W-1)); y = max(0, min(y, H-1))
        w = max(1, min(w, W-x));  h = max(1, min(h, H-y))
        crop = im.crop((x,y,x+w,y+h)).convert("RGB")
        crop.thumbnail((tw, th))

        cell = Image.new("RGB", (tw, th), BG_COLOR)
        off_x = (tw - crop.width)//2
        off_y = (th - crop.height)//2
        cell.paste(crop, (off_x, off_y))

        if label_text:
            d = ImageDraw.Draw(cell)
            try:
                bbox = d.textbbox((0,0), label_text)
                txt_w, txt_h = bbox[2]-bbox[0], bbox[3]-bbox[1]
            except Exception:
                txt_w, txt_h = d.textsize(label_text)

            pad_l, pad_r, pad_t, pad_b = 2, 2, 0, 2
            x1 = max(0, (cell.width - (txt_w + pad_l + pad_r)) // 2)
            y1 = 0
            x2 = min(cell.width-1, x1 + txt_w + pad_l + pad_r - 1)
            y2 = y1 + txt_h + pad_t + pad_b - 1
            d.rectangle([x1, y1, x2, y2], fill=(0,255,0))  # 배경 녹색
            d.text((x1 + pad_l, y1 + pad_t), label_text, fill=(0,0,0))  # 텍스트 검은색
        return cell

def show_contact_sheet_flat(items, cols=COLS, tw=TILE_W, th=TILE_H, max_rows=SHEET_MAX_ROWS):
    """시각화: 20열 직렬 시트 (페이지 분할)"""
    if not items:
        return
    cols = max(1, cols)
    rows_per_sheet = max_rows if max_rows is not None else math.ceil(len(items)/cols)
    per_sheet = cols * rows_per_sheet
    total = len(items)
    pages = math.ceil(total / per_sheet)

    for pg in range(pages):
        chunk = items[pg*per_sheet : (pg+1)*per_sheet]
        rows = math.ceil(len(chunk)/cols)
        sheet = Image.new("RGB", (cols*tw, rows*th), BG_COLOR)
        for i, (p, lab) in enumerate(chunk):
            r, c = divmod(i, cols)
            try:
                cell = make_tile(p, lab, tw, th)
            except Exception:
                cell = Image.new("RGB", (tw, th), (60,60,60))
            sheet.paste(cell, (c*tw, r*th))
        display(sheet)

# KEEP/DROP 세트 준비
KEEP_PATHS_SET = set(globals().get("KEEP_PATHS_SET", set()))
DROP_PATHS_SET = set(globals().get("DROP_PATHS_SET", set()))
_norm = lambda p: str(Path(p).resolve())

# 메인: 레이블별 파라미터로 클러스터링 → 평탄화 시퀀스 생성 + KEEP/DROP 갱신
for cls in CLASSES:
    paths = by_class[cls]
    images_count = len(paths)
    if images_count == 0:
        print(f"[{cls}] clusters: 0 | images: 0 | keep-images: 0 | Drop-images: 0")
        continue

    feats = embed_paths(paths)

    sim_thr_cls = PER_CLASS_SIM_THR.get(cls, SIM_THR_DEFAULT)
    nn_k_cls    = PER_CLASS_NN_K.get(cls,    NN_K_DEFAULT)

    _, clusters = union_find_clusters(feats, sim_thr=sim_thr_cls, nn_k=nn_k_cls)

    keep_idx = pick_keep_indices(paths, clusters, k=K_PER_CLUSTER)
    keep_set = set(keep_idx)

    # KEEP/DROP 경로 집합 갱신
    keep_paths = [paths[i] for i in keep_idx]
    drop_paths = [paths[i] for i in range(len(paths)) if i not in keep_set]
    KEEP_PATHS_SET.update(_norm(p) for p in keep_paths)
    DROP_PATHS_SET.update(_norm(p) for p in drop_paths)

    # Drop 총합
    total_drop = len(drop_paths)

    # 클러스터를 "Ref의 시간" 기준으로 정렬 (이른 샷 먼저)
    cl_list = list(clusters.items())
    def _ref_idx_and_time(members):
        ref_i = min(members, key=lambda m: parse_time_seconds_from_name(paths[m]))
        ref_t = parse_time_seconds_from_name(paths[ref_i])
        return ref_i, ref_t
    cl_list.sort(key=lambda kv: _ref_idx_and_time(kv[1])[1])

    # 평탄화 시퀀스: Ref 1장(라벨) + Drop ≤3장(무라벨)
    flat_items = []
    shown_rank = 0
    for _, members in cl_list:
        if not members:
            continue
        ref_idx = min(members, key=lambda m: parse_time_seconds_from_name(paths[m]))
        drops = [m for m in members if m != ref_idx]
        drop_count = len(drops)
        if drop_count == 0:
            continue

        shown_rank += 1
        ref_path = paths[ref_idx]
        ref_label = f"C#{shown_rank}_Drop_{int(drop_count)}"
        flat_items.append((ref_path, ref_label))

        # Drop 타일: 최대 3장
        if drop_count <= 3:
            drop_paths_show = [paths[i] for i in drops]
        else:
            drop_paths_show = [paths[i] for i in random.sample(drops, k=3)]
        for dp in drop_paths_show:
            flat_items.append((dp, None))

    # 요약
    keep_images = len(keep_idx)
    print(
        f"[{cls}] clusters: {len(clusters)} | images: {images_count} "
        f"| keep-images: {keep_images} | Drop-images: {total_drop}"
    )

    # 시각화
    show_contact_sheet_flat(flat_items, cols=COLS, tw=TILE_W, th=TILE_H, max_rows=SHEET_MAX_ROWS)

# 정리
del feat_model
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
# === [/### 2-1] ===

### 3. 변환(Transforms)
from torchvision import transforms as T
from PIL import Image

# ROI Crop 변환
class CropROI:
    def __init__(self, xywh):
        self.x, self.y, self.w, self.h = map(int, xywh)
    def __call__(self, img: Image.Image):
        W, H = img.size
        x = max(0, min(self.x, W-1))
        y = max(0, min(self.y, H-1))
        w = max(1, min(self.w, W - x))
        h = max(1, min(self.h, H - y))
        return img.crop((x, y, x + w, y + h))

# 기본 정규화
imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std  = [0.229, 0.224, 0.225]

# 학습 변환 (ROI Crop → Resize → Augment → ToTensor → Normalize)
train_tfms = T.Compose([
    CropROI(ROI_RECT),
    T.Grayscale(num_output_channels=3),
    T.Resize((IMG_SIZE, IMG_SIZE), interpolation=T.InterpolationMode.BILINEAR),
    T.RandomHorizontalFlip(p=0.5),
    T.RandomAffine(degrees=4, translate=(0.02,0.02), scale=(0.98,1.02)),
    T.ToTensor(),
    T.Normalize(mean=imagenet_mean, std=imagenet_std),
])

# 검증/테스트 변환 (ROI Crop → Resize → ToTensor → Normalize)
valtest_tfms = T.Compose([
    CropROI(ROI_RECT),
    T.Grayscale(num_output_channels=3),
    T.Resize((IMG_SIZE, IMG_SIZE), interpolation=T.InterpolationMode.BILINEAR),
    T.ToTensor(),
    T.Normalize(mean=imagenet_mean, std=imagenet_std),
])
# === [/Cell 3] ===

### 4. 데이터셋 로드 & 층화 분할
from torchvision import datasets
import torch, random
from torch.utils.data import Dataset
from pathlib import Path
from collections import defaultdict, Counter

def _norm_path(p) -> str:
    """경로 표준화(절대경로, OS 구분자 통일)"""
    return str(Path(p).resolve())

# 전체 데이터 로드
dataset_full = datasets.ImageFolder(root=DATA_ROOT)
idx_to_class = {v: k for k, v in dataset_full.class_to_idx.items()}
print("[INFO] classes:", idx_to_class)

N = len(dataset_full.samples)
base_paths_norm = [_norm_path(p) for p, _ in dataset_full.samples]
base_targets = [int(c) for _, c in dataset_full.samples]  # 리스트(정수)

# 셀 2-1의 중복 제거 결과 반영
keep_paths_set = globals().get("KEEP_PATHS_SET", None)
drop_paths_set = globals().get("DROP_PATHS_SET", None)

filtered_indices = None

if keep_paths_set and len(keep_paths_set) > 0:
    keep_set_norm = {_norm_path(p) for p in keep_paths_set}
    keep_mask = [p in keep_set_norm for p in base_paths_norm]
    filtered_indices = [i for i, m in enumerate(keep_mask) if m]
    if len(filtered_indices) == 0:
        raise RuntimeError("[ERR] 중복 제거 결과가 비어 있습니다. SIM_THR/K_PER_CLUSTER를 완화하세요.")
    print(f"[FILTER] keep {len(filtered_indices)}/{N} images after near-duplicate removal (KEEP set).")

elif (not keep_paths_set) and drop_paths_set and len(drop_paths_set) > 0:
    drop_set_norm = {_norm_path(p) for p in drop_paths_set}
    filtered_indices = [i for i, p in enumerate(base_paths_norm) if p not in drop_set_norm]
    print(f"[FILTER] keep {len(filtered_indices)}/{N} images after excluding DROP set ({len(drop_paths_set)} drops).")

else:
    filtered_indices = list(range(N))
    print(f"[FILTER] KEEP/DROP sets not provided → using all {N} samples.")

# 클래스 분포(필터 적용 후) 확인
filtered_targets = [base_targets[i] for i in filtered_indices]
cnt = Counter(filtered_targets)
print("[INFO] class counts after filter:", {idx_to_class[k]: v for k, v in cnt.items()})

# 한 클래스가 0이면 학습 불가
missing = [idx_to_class[k] for k in idx_to_class if k not in cnt or cnt[k] == 0]
if missing:
    raise RuntimeError(f"[ERR] 필터링 후 다음 클래스가 0개입니다: {missing}. " "ROI/중복제거 기준 또는 데이터 구성을 확인하세요.")

def stratified_split_indices_from_indices(indices, targets_list,
                                          train_ratio=0.7, val_ratio=0.15, seed=42):
    """층화 분할(필터된 샘플들에 대해 수행)"""
    by_cls = defaultdict(list)
    for idx, y in zip(indices, targets_list):
        by_cls[int(y)].append(idx)

    rng = random.Random(seed)
    train_idx, val_idx, test_idx = [], [], []
    for cls, idxs in by_cls.items():
        rng.shuffle(idxs)
        n = len(idxs)
        n_train = int(round(n * train_ratio))
        n_val   = int(round(n * val_ratio))
        cls_train = idxs[:n_train]
        cls_val   = idxs[n_train:n_train + n_val]
        cls_test  = idxs[n_train + n_val:]
        train_idx.extend(cls_train); val_idx.extend(cls_val); test_idx.extend(cls_test)

    rng.shuffle(train_idx); rng.shuffle(val_idx); rng.shuffle(test_idx)
    return train_idx, val_idx, test_idx

train_idx, val_idx, test_idx = stratified_split_indices_from_indices(
    filtered_indices, filtered_targets, train_ratio=0.7, val_ratio=0.15, seed=42
)

print(f"[SPLIT] train={len(train_idx)}  val={len(val_idx)}  test={len(test_idx)}")

class TransformSubset(Dataset):
    """서브셋 래퍼(셀3의 변환 적용)"""
    def __init__(self, base_ds, indices, transform=None):
        self.base_ds = base_ds
        self.indices = list(indices)
        self.transform = transform
    def __len__(self):
        return len(self.indices)
    def __getitem__(self, i):
        img, label = self.base_ds[self.indices[i]]
        if self.transform is not None:
            img = self.transform(img)
        return img, label

# 셀3에서 정의한 변환 존재 확인
assert 'train_tfms' in globals(),  "[ERR] 셀3의 train_tfms가 필요합니다."
assert 'valtest_tfms' in globals(), "[ERR] 셀3의 valtest_tfms가 필요합니다."

train_ds = TransformSubset(dataset_full, train_idx, transform=train_tfms)
val_ds   = TransformSubset(dataset_full, val_idx,   transform=valtest_tfms)
test_ds  = TransformSubset(dataset_full, test_idx,  transform=valtest_tfms)

print(f"[INFO] splits(final) -> train:{len(train_ds)}  val:{len(val_ds)}  test:{len(test_ds)}")
# === [/### 4] ===

### 5. DataLoader
import os, torch

EFFECTIVE_NUM_WORKERS = 0 if os.name == "nt" else NUM_WORKERS
PIN_MEMORY_EFF = bool(torch.cuda.is_available())
PERSISTENT = EFFECTIVE_NUM_WORKERS > 0

_common = dict(num_workers=EFFECTIVE_NUM_WORKERS, pin_memory=PIN_MEMORY_EFF, persistent_workers=PERSISTENT)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, **_common)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, **_common)
test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, **_common)

print("[INFO] train/val/test lens:", len(train_ds), len(val_ds), len(test_ds))

### 6. 모델 & 옵티마 & 스케줄러 + AMP
import math
import numpy as np
import torch
import torch.nn.functional as F

# ViT 수기 Attention Rollout
@torch.no_grad()
def attention_rollout_manual(sample_t, model, device, discard_ratio=0.0):
    """ViT 수기 Attention Rollout"""
    model.eval()
    x = sample_t.to(device, non_blocking=True)

    # 각 블록의 attn 입력(=norm1(x))을 pre_hook으로 수집
    attn_inputs = []
    hooks = []
    attn_modules = []
    for blk in model.blocks:
        attn_modules.append(blk.attn)
    def _pre_hook(module, inputs):
        attn_inputs.append(inputs[0].detach())
    for m in attn_modules:
        hooks.append(m.register_forward_pre_hook(_pre_hook))

    # 1회 forward: logits & attn_inputs 획득
    logits = model(x)
    pred = int(logits.argmax(dim=1).item())

    for h in hooks:
        h.remove()

    # 레이어별 어텐션 행렬 계산
    A_list = []
    for (m, xin) in zip(attn_modules, attn_inputs):
        B, N, C = xin.shape
        xin = xin.to(device)
        qkv = m.qkv(xin)
        qkv = qkv.reshape(B, N, 3, m.num_heads, C // m.num_heads)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        attn = (q @ k.transpose(-2, -1)) * m.scale
        attn = attn.softmax(dim=-1)
        attn = attn.mean(dim=1)
        A_list.append(attn[0])

    # Rollout (행렬 정규화 후 층별 행렬곱 누적
    R = None
    for A in A_list:
        N = A.shape[-1]
        A = A + torch.eye(N, device=A.device)
        A = A / A.sum(dim=-1, keepdim=True)
        R = A if R is None else (A @ R)

    # CLS(0) → Patch(1:) 주의도 추출 후 입력 해상도로 업샘플
    cls_to_patch = R[0, 1:]
    g = int(math.sqrt(cls_to_patch.numel()))
    heat = cls_to_patch.reshape(1, 1, g, g)
    Ht, Wt = sample_t.shape[-2], sample_t.shape[-1]
    heat_up = F.interpolate(heat, size=(Ht, Wt), mode="bilinear", align_corners=False)
    heat_up = heat_up[0, 0].detach().cpu().float().numpy()
    return heat_up, pred

def normalize_heat(heat, perc=99.0, gamma=1.0):
    """Heatmap 정규화(퍼센타일 클리핑 + 감마)"""
    if torch.is_tensor(heat):
        h = heat.detach().cpu().float().numpy()
    else:
        h = np.asarray(heat, dtype=np.float32)
    h = np.maximum(h, 0.0)
    vmax = np.percentile(h, perc) if perc is not None else h.max()
    vmax = max(float(vmax), 1e-6)
    h = np.clip(h / vmax, 0.0, 1.0)
    if gamma is not None and gamma != 1.0:
        h = np.power(h, gamma)
    return h
# === [/### 6] ===

### 7. 학습/검증 루프 + Early Stopping + Best Save/Load
from time import time
from tqdm.auto import tqdm
from IPython.display import clear_output, display
import matplotlib.pyplot as plt
from torch.amp import autocast, GradScaler
import torch, numpy as np, copy

# ------------------- 정책/옵션 -------------------
SAVE_CRITERION = 'val_loss'       # 'val_loss' 권장. ('val_acc'도 가능)
USE_TIE_BREAKERS = True
MIN_DELTA = 1e-4                  # 개선으로 볼 최소 변화량

USE_EMA = True                    # EMA 가중치 유지/평가/저장
EMA_DECAY = 0.999                 # 0.999~0.9999 권장
SAVE_BEST_PATH = globals().get('SAVE_BEST_PATH', 'vit_icing_melted_standard_best.pth')
SAVE_BEST_EMA_PATH = 'vit_icing_melted_best_ema.pth'

SHOW_LIVE_PLOT = False

#  환경/AMP
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device_type = "cuda" if torch.cuda.is_available() else "cpu"
use_amp = (device_type == "cuda")
amp_dtype = torch.float16 if device_type == "cuda" else torch.bfloat16
if 'scaler' not in globals():
    scaler = GradScaler(device='cuda', enabled=use_amp)

def _infer_num_classes_from_loader(loader):
    """의존성 자동 보완"""
    try:
        _, ytmp = next(iter(loader))
        return int(ytmp.max().item()) + 1
    except Exception:
        return None

def _infer_num_classes_from_ds():
    if 'train_ds' in globals():
        ds = globals()['train_ds']
        try:
            if hasattr(ds, 'dataset') and hasattr(ds.dataset, 'classes'):
                return len(ds.dataset.classes)
            if hasattr(ds, 'classes'):
                return len(ds.classes)
        except Exception:
            pass
    return None

def ensure_model_and_optim():
    global model, criterion, optimizer, scheduler
    num_classes = None
    if 'train_loader' in globals():
        num_classes = _infer_num_classes_from_loader(globals()['train_loader'])
    if num_classes is None:
        num_classes = _infer_num_classes_from_ds()
    if num_classes is None:
        num_classes = 2
        print("[WARN] Could not infer num_classes; defaulting to 2.")

    if 'model' not in globals():
        import timm
        name = globals().get('MODEL_NAME', 'vit_small_patch16_224')
        pretrained = globals().get('USE_PRETRAINED', True)
        drop = globals().get('DROP_PATH_RATE', 0.0)
        model = timm.create_model(name, pretrained=pretrained,
                                  num_classes=num_classes, drop_path_rate=drop)
    model.to(device)

    if 'criterion' not in globals():
        criterion = torch.nn.CrossEntropyLoss()

    if 'optimizer' not in globals():
        lr = globals().get('BASE_LR', 5e-5)
        wd = globals().get('WEIGHT_DECAY', 0.05)
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)

    if 'scheduler' not in globals():
        total = globals().get('EPOCHS', 20)
        warm  = globals().get('WARMUP_EPOCHS', 0)
        T_max = max(1, total - warm)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)

assert 'train_loader' in globals() and 'val_loader' in globals(), \
    "[ERR] train_loader/val_loader가 필요합니다. (셀5 실행)"
ensure_model_and_optim()

@torch.no_grad()
def evaluate_full(model, dataloader, device, n_bins_ece=15):
    """평가 지표 유틸 (loss/acc + confusion → macro_f1, min_recall + ECE) 계산"""
    model.eval()
    total_loss, total_samples, total_correct = 0.0, 0, 0
    all_probs = []
    all_targets = []

    nc = _infer_num_classes_from_loader(dataloader)
    if nc is None: nc = _infer_num_classes_from_ds() or 2
    conf = torch.zeros(nc, nc, dtype=torch.long)

    for xb, yb in dataloader:
        xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)
        with autocast(device_type=device_type, dtype=amp_dtype, enabled=False):
            logits = model(xb)
            loss = criterion(logits, yb)
        probs = torch.softmax(logits, dim=1)
        preds = probs.argmax(dim=1)

        total_loss += loss.item() * yb.size(0)
        total_correct += (preds == yb).sum().item()
        total_samples += yb.size(0)

        all_probs.append(probs.detach().cpu())
        all_targets.append(yb.detach().cpu())

        with torch.no_grad():
            for t, p in zip(yb.view(-1), preds.view(-1)):
                conf[t.long(), p.long()] += 1

    # aggregate
    loss = total_loss / max(1, total_samples)
    acc  = total_correct / max(1, total_samples)
    probs = torch.cat(all_probs, dim=0) if all_probs else torch.empty(0, nc)
    targets = torch.cat(all_targets, dim=0) if all_targets else torch.empty(0, dtype=torch.long)

    # per-class precision/recall, macro F1
    tp = conf.diag().float()
    per_cls_pred = conf.sum(dim=0).float().clamp(min=1)
    per_cls_true = conf.sum(dim=1).float().clamp(min=1)
    precision = tp / per_cls_pred
    recall    = tp / per_cls_true
    f1_per = (2 * precision * recall / (precision + recall).clamp(min=1e-12))
    macro_f1 = float(torch.nan_to_num(f1_per, nan=0.0).mean().item())
    min_recall = float(torch.nan_to_num(recall, nan=0.0).min().item())

    # ECE (Expected Calibration Error)
    ece = 0.0
    if probs.numel() > 0:
        confidences, pred_labels = probs.max(dim=1)
        correctness = (pred_labels == targets).float()
        # equal-width bins in [0,1]
        bins = torch.linspace(0, 1, n_bins_ece + 1)
        ece_total = 0.0
        for i in range(n_bins_ece):
            lo, hi = bins[i].item(), bins[i+1].item()
            mask = (confidences >= lo) & (confidences < hi) if i < n_bins_ece - 1 else (confidences >= lo) & (confidences <= hi)
            if mask.any():
                conf_mean = confidences[mask].mean().item()
                acc_bin   = correctness[mask].mean().item()
                ece_total += (mask.float().mean().item()) * abs(conf_mean - acc_bin)
        ece = float(ece_total)

    return {
        "loss": loss,
        "acc": acc,
        "macro_f1": macro_f1,
        "min_recall": min_recall,
        "ece": ece,
        "confusion": conf.cpu().numpy(),
    }

def ema_update(ema_model, model, decay=0.999):
    """1-epoch 루프 (EMA 지원)"""
    with torch.no_grad():
        for p_ema, p in zip(ema_model.parameters(), model.parameters()):
            p_ema.data.mul_(decay).add_(p.data, alpha=1.0 - decay)

def run_one_epoch(model, loader, optimizer=None, device="cpu",
                  scaler=None, limit_batches=None, desc="train",
                  ema_model=None, ema_decay=0.999):
    is_train = optimizer is not None
    model.train(is_train)

    total_loss, total_correct, total_samples = 0.0, 0, 0
    pbar = tqdm(enumerate(loader),
                total=(len(loader) if limit_batches is None else min(limit_batches, len(loader))),
                leave=False, desc=desc)
    for step, (x, y) in pbar:
        if limit_batches is not None and step >= limit_batches:
            break
        x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)

        if is_train:
            optimizer.zero_grad(set_to_none=True)
            with autocast(device_type=device_type, dtype=amp_dtype, enabled=use_amp):
                logits = model(x)
                loss = criterion(logits, y)
            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()

            if ema_model is not None:
                ema_update(ema_model, model, decay=ema_decay)

        else:
            with torch.no_grad():
                with autocast(device_type=device_type, dtype=amp_dtype, enabled=use_amp):
                    logits = model(x)
                    loss = criterion(logits, y)

        with torch.no_grad():
            preds = logits.argmax(dim=1)
            correct = (preds == y).sum().item()
            bs = y.size(0)
            total_correct += correct
            total_samples += bs
            total_loss += loss.item() * bs

        pbar.set_postfix(loss=f"{total_loss/total_samples:.4f}",
                         acc=f"{total_correct/total_samples:.3f}")

    avg_loss = total_loss / max(1, total_samples)
    avg_acc  = total_correct / max(1, total_samples)
    return avg_loss, avg_acc, total_samples

def _is_improved(best_primary, cur_primary, mode='min', eps=MIN_DELTA):
    """체크포인트 선택 로직"""
    if best_primary is None:
        return True
    if mode == 'min':
        return (best_primary - cur_primary) > eps
    else:
        return (cur_primary - best_primary) > eps

def _tie_break_is_better(best_metrics, cur_metrics):
    if not USE_TIE_BREAKERS: return False
    keys = [('min_recall', +1), ('ece', -1), ('macro_f1', +1)]
    for k, sign in keys:
        a = best_metrics.get(k, None); b = cur_metrics.get(k, None)
        if a is None or b is None: continue
        if abs(a - b) > 1e-8:
            return (b - a) * sign > 0
    return False

def train_standard_with_progress(model, train_loader, val_loader, epochs=10, device="cpu", scaler=None, limit_train=None, limit_val=None, early_stop_patience=5, save_path=SAVE_BEST_PATH):
    """학습 루프"""
    local_scaler = scaler if scaler is not None else GradScaler(device='cuda', enabled=use_amp)

    # EMA 모델 준비
    ema_model = None
    if USE_EMA:
        ema_model = copy.deepcopy(model).to(device)
        for p in ema_model.parameters():
            p.requires_grad_(False)

    best_primary = None
    best_metrics = None
    best_source  = 'base'          # 'base' or 'ema'
    wait = 0

    history = {"train_loss": [], "train_acc": [], "val_loss": [], "val_acc": [], "lr": [], "epoch_time": []}

    epoch_bar = tqdm(range(1, epochs+1), desc="Epochs", position=0, leave=True)
    t_start_all = time()

    if SHOW_LIVE_PLOT:
        plt.figure(figsize=(8,3))

    for ep in epoch_bar:
        lr_now = optimizer.param_groups[0]['lr']
        t_ep = time()

        # train
        tr_loss, tr_acc, _ = run_one_epoch(
            model, train_loader, optimizer=optimizer, device=device,
            scaler=local_scaler, limit_batches=limit_train, desc=f"train e{ep}",
            ema_model=ema_model if USE_EMA else None, ema_decay=EMA_DECAY
        )

        # valid (base)
        base_metrics = evaluate_full(model, val_loader, device)

        # valid (ema)
        if USE_EMA and ema_model is not None:
            ema_metrics = evaluate_full(ema_model, val_loader, device)
            # 선택할 평가 대상
            chosen_metrics = ema_metrics if SAVE_CRITERION in ('val_loss', 'val_acc') else ema_metrics
            chosen_source = 'ema'
            # 기준 지표 선택
            cur_primary = chosen_metrics['loss'] if SAVE_CRITERION == 'val_loss' else (1.0 - chosen_metrics['loss']) # (acc 쓰고 싶으면 위 evaluate에서 acc 사용)
            # val_loss 기준이면 loss 사용, acc 기준이면 acc 사용
            if SAVE_CRITERION == 'val_acc':
                cur_primary = chosen_metrics['acc']
                mode = 'max'
            else:
                mode = 'min'
        else:
            chosen_metrics = base_metrics
            chosen_source = 'base'
            if SAVE_CRITERION == 'val_acc':
                cur_primary = base_metrics['acc']; mode = 'max'
            else:
                cur_primary = base_metrics['loss']; mode = 'min'

        # 개선/저장 판단
        improved = _is_improved(best_primary, cur_primary, mode=mode, eps=MIN_DELTA)
        if not improved and best_metrics is not None:
            # 동률 범위 내에서 타이브레이크
            if abs((best_primary if best_primary is not None else cur_primary) - cur_primary) <= MIN_DELTA:
                if _tie_break_is_better(best_metrics, chosen_metrics):
                    improved = True

        if improved:
            best_primary = cur_primary
            best_metrics = chosen_metrics
            best_source  = chosen_source

            # 저장 (선택된 소스의 가중치 저장)
            try:
                if best_source == 'ema' and USE_EMA and ema_model is not None:
                    torch.save(ema_model.state_dict(), SAVE_BEST_PATH)
                    torch.save(ema_model.state_dict(), SAVE_BEST_EMA_PATH)
                else:
                    state = {k: v.detach().cpu() for k, v in model.state_dict().items()}
                    torch.save(state, save_path)
                tqdm.write(f"[INFO] (epoch {ep}) Saved BEST by {SAVE_CRITERION}"
                           f" [{best_source}] → {save_path}")
            except Exception as e:
                tqdm.write(f"[WARN] Could not save checkpoint: {e}")
            wait = 0
        else:
            wait += 1

        # 스케줄러
        try:
            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                monitor = base_metrics['acc'] if SAVE_CRITERION == 'val_acc' else -base_metrics['loss']
                scheduler.step(monitor)
            else:
                scheduler.step()
        except NameError:
            pass

        # 기록/로그
        va_loss, va_acc = base_metrics['loss'], base_metrics['acc']
        history["train_loss"].append(tr_loss); history["train_acc"].append(tr_acc)
        history["val_loss"].append(va_loss);   history["val_acc"].append(va_acc)
        history["lr"].append(lr_now)
        ep_time = time() - t_ep; history["epoch_time"].append(ep_time)

        elapsed = time() - t_start_all
        remaining = (epochs - ep) * (elapsed / ep + 1e-8)
        epoch_bar.set_postfix({
            "lr": f"{lr_now:.2e}",
            "t/ep": f"{ep_time:.1f}s",
            "best_src": best_source,
            SAVE_CRITERION: f"{best_primary:.4f}" if isinstance(best_primary, float) else str(best_primary),
            "wait": f"{wait}/{early_stop_patience}"
        })

        tqdm.write(
            f"[Epoch {ep}/{epochs}] lr {lr_now:.2e} | "
            f"train {tr_loss:.4f}/{tr_acc:.3f} | "
            f"val(base) loss {base_metrics['loss']:.4f} acc {base_metrics['acc']:.3f} "
            + (f"| val(ema) loss {ema_metrics['loss']:.4f} acc {ema_metrics['acc']:.3f} " if USE_EMA else "")
            + (f"| best[{best_source}] {SAVE_CRITERION} {best_primary:.4f}" )
        )

        if SHOW_LIVE_PLOT:
            clear_output(wait=True)
            plt.clf()
            xs = range(1, len(history["train_loss"])+1)
            plt.subplot(1,2,1); plt.plot(xs, history["train_loss"], label="train")
            plt.plot(xs, history["val_loss"], label="val"); plt.title("Loss"); plt.legend(); plt.grid(True)
            plt.subplot(1,2,2); plt.plot(xs, history["train_acc"], label="train")
            plt.plot(xs, history["val_acc"], label="val"); plt.title("Accuracy"); plt.legend(); plt.grid(True)
            plt.tight_layout(); display(plt.gcf())

        # 조기 종료
        if early_stop_patience is not None and wait >= early_stop_patience:
            tqdm.write(f"[EARLY STOP] patience {early_stop_patience} 도달. 중단.")
            break

    # 종료 시, 저장된 best를 로드
    try:
        state = torch.load(SAVE_BEST_PATH, map_location='cpu')
        model.load_state_dict(state)
        model.to(device)
        tqdm.write(f"[INFO] Loaded best weights from {SAVE_BEST_PATH} ({best_source}).")
    except Exception as e:
        tqdm.write(f"[WARN] Could not reload best model: {e}")

    return history

# 실행
history = train_standard_with_progress(
    model, train_loader, val_loader,
    epochs=globals().get('EPOCHS', 20), device=device, scaler=scaler,
    limit_train=globals().get('LIMIT_TRAIN_BATCHES', None),
    limit_val=globals().get('LIMIT_VAL_BATCHES', None),
    early_stop_patience=5,
    save_path=globals().get('SAVE_BEST_PATH', 'vit_icing_melted_standard_best.pth')
)
# === [/### 7] ===

### 8. 학습 곡선 & 테스트 평가
# 학습 곡선
fig, ax = plt.subplots(1, 2, figsize=(10,4))
ax[0].plot(history["train_loss"], label="train"); ax[0].plot(history["val_loss"], label="val")
ax[0].set_title("Loss"); ax[0].legend(); ax[0].grid(True)
ax[1].plot(history["train_acc"], label="train"); ax[1].plot(history["val_acc"], label="val")
ax[1].set_title("Accuracy"); ax[1].legend(); ax[1].grid(True)
plt.show()

# 테스트 평가
model.eval()
total_correct, total_samples = 0, 0
all_preds, all_labels = [], []

with torch.no_grad():
    for x, y in test_loader:
        x = x.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)
        logits = model(x)
        preds = logits.argmax(dim=1)
        total_correct += (preds == y).sum().item()
        total_samples += y.size(0)
        all_preds.append(preds.cpu())
        all_labels.append(y.cpu())

test_acc = total_correct / max(1, total_samples)
print(f"[TEST] Accuracy: {test_acc:.3f}")

# 리포트/혼동행렬
try:
    from sklearn.metrics import classification_report, confusion_matrix
    y_true = torch.cat(all_labels).numpy()
    y_pred = torch.cat(all_preds).numpy()
    print("\n[TEST] classification report")
    target_names = [idx_to_class[0], idx_to_class[1]]
    print(classification_report(y_true, y_pred, target_names=target_names))

    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(1,1, figsize=(4,4))
    im = ax.imshow(cm, cmap="Blues")
    ax.set_xticks([0,1]); ax.set_yticks([0,1])
    ax.set_xticklabels(target_names); ax.set_yticklabels(target_names)
    ax.set_xlabel("Predicted"); ax.set_ylabel("True")
    for (i,j), v in np.ndenumerate(cm):
        ax.text(j, i, str(v), ha='center', va='center', color='black')
        
    plt.title("Confusion Matrix")
    plt.tight_layout()
    plt.show()
except Exception as e:
    print("[WARN] sklearn not available or error -> skipping report/matrix.", e)

### 9. (검증)ViT Attention Rollout
import torch, numpy as np, matplotlib.pyplot as plt
import torch.nn.functional as F

@torch.no_grad()
def attention_rollout_manual(x_bchw, model, device, discard_ratio=0.0):
    x = x_bchw.to(device)
    model.eval()

    # 예측 클래스
    pred = int(model(x).argmax(dim=1).item())

    # 토큰 준비
    tokens = model.patch_embed(x)
    B, N, C = tokens.shape

    if hasattr(model, "cls_token") and model.cls_token is not None:
        cls_tok = model.cls_token.expand(B, -1, -1)
        tokens = torch.cat((cls_tok, tokens), dim=1)

    if hasattr(model, "pos_embed") and model.pos_embed is not None:
        tokens = tokens + model.pos_embed

    if hasattr(model, "pos_drop") and model.pos_drop is not None:
        tokens = model.pos_drop(tokens)

    # 각 블록의 어텐션 행렬 복원 (qkv로 계산)
    attn_list = []
    for blk in model.blocks:
        t_norm = blk.norm1(tokens)

        attn_mod = blk.attn
        qkv = attn_mod.qkv(t_norm)
        qkv = qkv.reshape(B, t_norm.shape[1], 3, attn_mod.num_heads, C // attn_mod.num_heads)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        scale = getattr(attn_mod, "scale", (q.shape[-1] ** -0.5))
        attn = (q @ k.transpose(-2, -1)) * scale
        attn = attn.softmax(dim=-1)
        attn_list.append(attn.detach().cpu())

        tokens = blk(tokens)

    # Rollout 누적 (head 평균 -> 정규화 -> 행렬곱 누적)
    joint = None
    for A in attn_list:
        A = A.mean(dim=1)[0]
        N_all = A.size(-1)
        A = A + torch.eye(N_all, device=A.device)
        A = A / A.sum(dim=-1, keepdim=True)
        joint = A if joint is None else joint @ A

    cls_attn = joint[0, 1:]
    if hasattr(model, "patch_embed") and hasattr(model.patch_embed, "grid_size"):
        gh, gw = model.patch_embed.grid_size
    else:
        L = int(np.sqrt(cls_attn.numel()))
        gh = gw = L

    heat = cls_attn.reshape(int(gh), int(gw))
    # discard_ratio로 작은 값 일부 제거
    if discard_ratio > 0:
        flat = heat.flatten()
        k = int(discard_ratio * flat.numel())
        if 0 < k < flat.numel():
            thresh = torch.topk(flat, flat.numel()-k).values.min()
            heat = torch.clamp(heat - thresh, min=0)

    heat = heat / (heat.max() + 1e-8)
    heat = heat.unsqueeze(0).unsqueeze(0)
    H, W = x.shape[-2:]
    heat_up = F.interpolate(heat, size=(H, W), mode="bilinear", align_corners=False)[0,0]
    return heat_up.cpu().numpy(), pred

### 10. Attention 컨투어맵 오버레이 플로팅
import numpy as np, math, torch, matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from torchvision import transforms
from PIL import Image, ImageOps
import torch.nn.functional as F

# ------- Grid / Layout -------
GRID_ROWS        = 7       # 고정 행 수
COLS_LEFT        = 7       # 좌측 Melted 열 수
COLS_RIGHT       = 7       # 우측 Icing  열 수
COLS_TOTAL       = COLS_LEFT + COLS_RIGHT

# ------- 행 높이 / 열 폭 -------
ROW_HEIGHT_IN    = 2.8
COL_WIDTH_IN     = None  # None이면 샘플 종횡비 자동 추정해서 계산

# ------- 서브플롯 간격 -------
WSPACE           = 0.02
HSPACE           = 0.015
PAD_LEFT, PAD_RIGHT = 0.005, 0.995
PAD_TOP, PAD_BOTTOM = 0.995, 0.005
GUTTER_COL_RATIO = 0.3  # 가운데 스페이서 컬럼 폭 비율 (1.0 = 일반 컬럼 폭)

# ------- Contour / Heatmap -------
PERC_CLIP        = 98         # 상위 퍼센타일 클리핑
GAMMA            = 1.5        # 감마 보정
LOW_LEVELS       = [0.30]     # 컨투어 낮은 레벨
HIGH_LEVELS      = [0.95]     # 컨투어 높은 레벨
LOW_COLOR        = "yellow"
HIGH_COLOR       = "lime"
CONTOUR_LW       = 1.0     
ROLL_DISCARD     = 0.0        # attention rollout discard ratio

# ROI / Transform (fallback)
ROI_RECT = globals().get("ROI_RECT", (0, 0, 224, 224))
IMG_SIZE = int(globals().get("IMG_SIZE", 224))

# ImageNet 정규화 기본값
try:
    imagenet_mean, imagenet_std
except NameError:
    imagenet_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
    imagenet_std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)

class CropROI:
    def __init__(self, xywh): self.x, self.y, self.w, self.h = map(int, xywh)
    def __call__(self, img: Image.Image):
        img = ImageOps.exif_transpose(img)
        W, H = img.size
        x = max(0, min(self.x, W-1)); y = max(0, min(self.y, H-1))
        w = max(1, min(self.w, W-x)); h = max(1, min(self.h, H-y))
        return img.crop((x, y, x+w, y+h))

# 셀3의 valtest_tfms가 있으면 재사용, 없으면 안전 기본값 구성
val_transform = globals().get("valtest_tfms", None)
if val_transform is None:
    from torchvision.transforms import InterpolationMode
    val_transform = transforms.Compose([
        CropROI(ROI_RECT),
        transforms.Grayscale(num_output_channels=3),
        transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=InterpolationMode.BILINEAR),
        transforms.ToTensor(),
        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),
    ])

def normalize_heat(h, perc=PERC_CLIP, gamma=GAMMA, eps=1e-8):
    h = np.asarray(h, dtype=np.float32)
    hi = np.percentile(h, perc); h = h / max(hi, eps)
    h = np.clip(h, 0, 1)
    return h if gamma == 1.0 else h ** gamma

# 필요한 전역 존재 확인
assert 'attention_rollout_manual' in globals(), "[ERR] 'attention_rollout_manual' 필요(셀6)."
class_to_idx = {v: k for k, v in idx_to_class.items()}

def _find_idx(name: str):
    if name in class_to_idx: return class_to_idx[name]
    for k in class_to_idx:
        if k.lower() == name.lower(): return class_to_idx[k]
    for k in class_to_idx:
        if name.lower() in k.lower():  return class_to_idx[k]
    raise KeyError(f"클래스 '{name}' 없음. 현재: {list(class_to_idx.keys())}")

melted_idx = _find_idx("Melted")
icing_idx  = _find_idx("Icing")

# Build test index lists
base_ds = test_ds.base_ds if hasattr(test_ds, "base_ds") else dataset_full
test_base_indices = list(test_ds.indices)

def _sorted_by_time(base_indices, class_idx):
    idxs = [i for i in base_indices if base_ds.targets[i] == class_idx]
    return sorted(idxs, key=lambda i: base_ds.samples[i][0])

melted_sorted = _sorted_by_time(test_base_indices, melted_idx)
icing_sorted  = _sorted_by_time(test_base_indices, icing_idx)

def _pick_uniform(lst, k):
    n = len(lst)
    if k <= 0 or n == 0: return []
    if k >= n: return lst[:]
    picks, prev = [], -1
    for p in np.linspace(0, n-1, num=k):
        idx = int(round(p)); idx = max(0, min(n-1, idx))
        if idx == prev and idx+1 < n: idx += 1
        picks.append(idx); prev = idx
    return [lst[i] for i in picks]

need_per_class = GRID_ROWS * COLS_LEFT
melted_pick = _pick_uniform(melted_sorted, need_per_class)
icing_pick  = _pick_uniform(icing_sorted,  need_per_class)

left_n, right_n = len(melted_pick), len(icing_pick)
rows_m = math.ceil(left_n  / COLS_LEFT) if left_n  > 0 else 0
rows_i = math.ceil(right_n / COLS_RIGHT) if right_n > 0 else 0
row_blocks = min(GRID_ROWS, max(rows_m, rows_i))
if row_blocks == 0:
    raise RuntimeError("표시할 샘플이 부족합니다. test split/파일 정렬을 확인하세요.")

def _infer_aspect_for_column():
    """Figure size (가능한 실제 원본 비율 추정)"""
    try:
        idx = (melted_pick + icing_pick)[0]
    except Exception:
        idx = test_base_indices[0]
    path = base_ds.samples[idx][0]
    with Image.open(path) as pil:
        pil = ImageOps.exif_transpose(pil)
        W0, H0 = pil.size
    return max(0.1, W0 / H0)

col_aspect = _infer_aspect_for_column()
col_w_in   = COL_WIDTH_IN if COL_WIDTH_IN is not None else (ROW_HEIGHT_IN * col_aspect)

# 가운데 스페이서 컬럼 폭까지 포함해서 fig 폭 계산
fig_w = col_w_in * (COLS_TOTAL + GUTTER_COL_RATIO)
fig_h = ROW_HEIGHT_IN * row_blocks

# Make grid with center spacer
fig = plt.figure(figsize=(fig_w, fig_h), constrained_layout=False)
widths = [1]*COLS_LEFT + [GUTTER_COL_RATIO] + [1]*COLS_RIGHT
gs = gridspec.GridSpec(row_blocks, COLS_TOTAL + 1, figure=fig, width_ratios=widths, wspace=WSPACE, hspace=HSPACE)
fig.subplots_adjust(left=PAD_LEFT, right=PAD_RIGHT, top=PAD_TOP, bottom=PAD_BOTTOM)

# 실제 이미지를 그릴 axes 배열
axes = np.empty((row_blocks, COLS_TOTAL), dtype=object)
for r in range(row_blocks):
    # 왼쪽 5열
    for c in range(COLS_LEFT):
        axes[r, c] = fig.add_subplot(gs[r, c])
    # 오른쪽 5열
    for c in range(COLS_RIGHT):
        axes[r, COLS_LEFT + c] = fig.add_subplot(gs[r, COLS_LEFT + 1 + c])

# Helpers
pil_loader = getattr(base_ds, "loader", None)

def _get_input_and_path(base_idx):
    path = base_ds.samples[base_idx][0]
    img  = pil_loader(path) if pil_loader is not None else Image.open(path).convert("RGB")
    img  = ImageOps.exif_transpose(img)
    x    = val_transform(img)
    return x.unsqueeze(0), path

def _draw_on_original(ax, sample_t, orig_path, roi_xywh):
    heat, _ = attention_rollout_manual(sample_t, model, device, discard_ratio=ROLL_DISCARD)
    heat_n  = normalize_heat(heat, perc=PERC_CLIP, gamma=GAMMA)

    with Image.open(orig_path) as pil:
        pil = ImageOps.exif_transpose(pil).convert("RGB")
        W0, H0 = pil.size
        extent = (0, W0, H0, 0)

        x, y, w, h = map(int, roi_xywh)
        x = max(0, min(x, W0-1)); y = max(0, min(y, H0-1))
        w = max(1, min(w, W0-x));  h = max(1, min(h, H0-y))

        heat_t   = torch.from_numpy(heat_n)[None, None]
        heat_roi = F.interpolate(heat_t, size=(h, w), mode='bilinear', align_corners=False)[0, 0].numpy()

        canvas = np.zeros((H0, W0), dtype=np.float32)
        canvas[y:y+h, x:x+w] = heat_roi

        ax.imshow(pil, origin='upper', extent=extent, aspect='equal')
        if LOW_LEVELS:
            ax.contour(canvas, levels=LOW_LEVELS, colors=LOW_COLOR,
                       linewidths=CONTOUR_LW, alpha=0.95,
                       origin='upper', extent=extent)
        if HIGH_LEVELS:
            ax.contour(canvas, levels=HIGH_LEVELS, colors=HIGH_COLOR,
                       linewidths=CONTOUR_LW, alpha=0.95,
                       origin='upper', extent=extent)

        ax.set_xlim(0, W0); ax.set_ylim(H0, 0)
        ax.set_axis_off()

# # Render grid: 좌_Melted
for r in range(row_blocks):
    for c in range(COLS_LEFT):
        k = r*COLS_LEFT + c
        if k >= left_n: break
        base_idx = melted_pick[k]
        sample_t, path = _get_input_and_path(base_idx)
        _draw_on_original(axes[r, c], sample_t.to(device), path, ROI_RECT)

# Render grid: 우_Icing
for r in range(row_blocks):
    for c in range(COLS_RIGHT):
        k = r*COLS_RIGHT + c
        if k >= right_n: break
        base_idx = icing_pick[k]
        sample_t, path = _get_input_and_path(base_idx)
        _draw_on_original(axes[r, COLS_LEFT + c], sample_t.to(device), path, ROI_RECT)

plt.show()
# === [/### 10] ===
